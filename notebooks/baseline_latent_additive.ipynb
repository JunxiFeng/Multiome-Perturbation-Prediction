{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AnnData: AnnData object with n_obs × n_vars = 21700 × 3332\n",
      "    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'seurat_clusters', 'Assign', 'scds', 'cxds', 'bcds', 'Sample', 'nCount_refAssay', 'nFeature_refAssay', 'predicted.subclass.score', 'predicted.subclass', 'CT', 'mito', 'BioSamp', 'CT2', 'ForPlot', 'Remove', 'active_ident', 'Assign_clean', 'condition', 'cell_type', 'cell_class', 'celltype_mapped', 'split'\n",
      "    var: 'variable_gene', 'gene_name', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'gene_name_upper'\n",
      "    uns: 'ATAC_embeddings', 'GET_embeddings', 'hvg', 'log1p'\n",
      "Split counts:\n",
      "split\n",
      "train    16070\n",
      "test      3033\n",
      "val       2597\n",
      "Name: count, dtype: int64\n",
      "Train: 16070 | Val: 2597 | Test: 3033\n",
      "Genes: 3332\n",
      "Perturbations: ['ANK3', 'BCL11B', 'CUL1', 'CX3CL1', 'DAB1', 'HERC1', 'RB1CC1', 'SATB2', 'TBR1', 'TRIO', 'XPO7', 'ctrl']\n",
      "Pert dim: 12\n",
      "Covariates dim: 4\n",
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "LatentAdditive model training using external\n",
    "train/val/test split file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================\n",
    "# 0️⃣ Reproducibility setup\n",
    "# =========================================\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =========================================\n",
    "# 1️⃣ Load full AnnData (NOT control-only)\n",
    "# =========================================\n",
    "adata_path = \"/gpfs/home/junxif/xin_lab/perturbench/data/boli_anndata/boli_with_GETembedding_celltypeaware_subset.h5ad\"\n",
    "split_path = \"/gpfs/home/junxif/xin_lab/perturbench/data/boli_251006_1_qual_high_amt_high_split.csv\"\n",
    "\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "\n",
    "print(\"Loaded AnnData:\", adata)\n",
    "\n",
    "# =========================================\n",
    "# 2️⃣ Load split CSV & attach to AnnData\n",
    "# =========================================\n",
    "df_split = pd.read_csv(split_path, header=None, names=[\"barcode\", \"split\"])\n",
    "df_split.index = df_split[\"barcode\"]\n",
    "\n",
    "adata.obs[\"split\"] = adata.obs.index.map(df_split[\"split\"])\n",
    "\n",
    "print(\"Split counts:\")\n",
    "print(adata.obs[\"split\"].value_counts())\n",
    "\n",
    "# =========================================\n",
    "# 3️⃣ Build train/val/test AnnData subsets\n",
    "# =========================================\n",
    "train_adata = adata[adata.obs[\"split\"] == \"train\"].copy()\n",
    "val_adata   = adata[adata.obs[\"split\"] == \"val\"].copy()\n",
    "test_adata  = adata[adata.obs[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(f\"Train: {train_adata.n_obs} | Val: {val_adata.n_obs} | Test: {test_adata.n_obs}\")\n",
    "print(f\"Genes: {adata.n_vars}\")\n",
    "\n",
    "# =========================================\n",
    "# 4️⃣ Helper to convert X to numpy\n",
    "# =========================================\n",
    "def to_numpy(X):\n",
    "    return X.toarray() if not isinstance(X, np.ndarray) else X\n",
    "\n",
    "X_train = to_numpy(train_adata.X)\n",
    "X_val   = to_numpy(val_adata.X)\n",
    "X_test  = to_numpy(test_adata.X)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# =========================================\n",
    "# 5️⃣ Perturbation one-hot for train/val/test\n",
    "# =========================================\n",
    "pert = adata.obs[\"condition\"].astype(\"category\")\n",
    "pert_onehot = pd.get_dummies(pert)\n",
    "\n",
    "p_train = torch.tensor(\n",
    "    pert_onehot.loc[train_adata.obs.index].values, dtype=torch.float32\n",
    ")\n",
    "p_val = torch.tensor(\n",
    "    pert_onehot.loc[val_adata.obs.index].values, dtype=torch.float32\n",
    ")\n",
    "p_test = torch.tensor(\n",
    "    pert_onehot.loc[test_adata.obs.index].values, dtype=torch.float32\n",
    ")\n",
    "\n",
    "n_perts = p_train.shape[1]\n",
    "print(\"Perturbations:\", list(pert_onehot.columns))\n",
    "print(\"Pert dim:\", n_perts)\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ Celltype covariates one-hot\n",
    "# =========================================\n",
    "celltypes = adata.obs[\"celltype_mapped\"].astype(\"category\")\n",
    "cov_onehot = pd.get_dummies(celltypes)\n",
    "\n",
    "cov_train = torch.tensor(cov_onehot.loc[train_adata.obs.index].values, dtype=torch.float32)\n",
    "cov_val   = torch.tensor(cov_onehot.loc[val_adata.obs.index].values,   dtype=torch.float32)\n",
    "cov_test  = torch.tensor(cov_onehot.loc[test_adata.obs.index].values,  dtype=torch.float32)\n",
    "\n",
    "n_cov = cov_train.shape[1]\n",
    "print(\"Covariates dim:\", n_cov)\n",
    "\n",
    "# reuse same covariates for encoder + decoder\n",
    "cov_train_enc = cov_train\n",
    "cov_train_dec = cov_train\n",
    "cov_val_enc   = cov_val\n",
    "cov_val_dec   = cov_val\n",
    "cov_test_enc  = cov_test\n",
    "cov_test_dec  = cov_test\n",
    "\n",
    "# =========================================\n",
    "# 7️⃣ Build PyTorch datasets\n",
    "# =========================================\n",
    "train_ds = TensorDataset(X_train, p_train, cov_train_enc, cov_train_dec)\n",
    "val_ds   = TensorDataset(X_val,   p_val,   cov_val_enc,   cov_val_dec)\n",
    "test_ds  = TensorDataset(X_test,  p_test,  cov_test_enc,  cov_test_dec)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17240/1885369313.py:64: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/20 (train):   0%|          | 0/251 [00:00<?, ?it/s]/tmp/ipykernel_17240/1885369313.py:80: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/20 (train): 100%|██████████| 251/251 [00:38<00:00,  6.56it/s]\n",
      "Epoch 1/20 (val):   0%|          | 0/41 [00:00<?, ?it/s]/tmp/ipykernel_17240/1885369313.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/20 (val): 100%|██████████| 41/41 [00:01<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train=0.088179 | val=0.066119 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 (train): 100%|██████████| 251/251 [00:31<00:00,  8.08it/s]\n",
      "Epoch 2/20 (val): 100%|██████████| 41/41 [00:01<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train=0.064604 | val=0.059377 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 (train): 100%|██████████| 251/251 [00:31<00:00,  8.00it/s]\n",
      "Epoch 3/20 (val): 100%|██████████| 41/41 [00:01<00:00, 31.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train=0.059164 | val=0.055553 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 (train): 100%|██████████| 251/251 [00:31<00:00,  8.04it/s]\n",
      "Epoch 4/20 (val): 100%|██████████| 41/41 [00:00<00:00, 69.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train=0.056165 | val=0.053702 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 (train): 100%|██████████| 251/251 [00:31<00:00,  8.01it/s]\n",
      "Epoch 5/20 (val): 100%|██████████| 41/41 [00:01<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train=0.054457 | val=0.052791 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.85it/s]\n",
      "Epoch 6/20 (val): 100%|██████████| 41/41 [00:00<00:00, 49.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train=0.053370 | val=0.051987 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.78it/s]\n",
      "Epoch 7/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train=0.052529 | val=0.051523 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.75it/s]\n",
      "Epoch 8/20 (val): 100%|██████████| 41/41 [00:01<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train=0.051830 | val=0.051018 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.68it/s]\n",
      "Epoch 9/20 (val): 100%|██████████| 41/41 [00:01<00:00, 28.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train=0.051173 | val=0.050531 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.80it/s]\n",
      "Epoch 10/20 (val): 100%|██████████| 41/41 [00:01<00:00, 30.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train=0.050615 | val=0.050265 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.71it/s]\n",
      "Epoch 11/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train=0.050112 | val=0.050066 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.83it/s]\n",
      "Epoch 12/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train=0.049635 | val=0.049842 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 (train): 100%|██████████| 251/251 [00:29<00:00,  8.65it/s]\n",
      "Epoch 13/20 (val): 100%|██████████| 41/41 [00:01<00:00, 30.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train=0.049193 | val=0.049595 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.89it/s]\n",
      "Epoch 14/20 (val): 100%|██████████| 41/41 [00:01<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train=0.048777 | val=0.049540 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.71it/s]\n",
      "Epoch 15/20 (val): 100%|██████████| 41/41 [00:01<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train=0.048347 | val=0.049329 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.75it/s]\n",
      "Epoch 16/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train=0.047941 | val=0.049111 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.78it/s]\n",
      "Epoch 17/20 (val): 100%|██████████| 41/41 [00:00<00:00, 43.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train=0.047550 | val=0.048998 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.85it/s]\n",
      "Epoch 18/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train=0.047157 | val=0.048891 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.75it/s]\n",
      "Epoch 19/20 (val): 100%|██████████| 41/41 [00:01<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train=0.046766 | val=0.048760 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 (train): 100%|██████████| 251/251 [00:28<00:00,  8.76it/s]\n",
      "Epoch 20/20 (val): 100%|██████████| 41/41 [00:01<00:00, 29.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train=0.046359 | val=0.048724 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5️⃣ LatentAdditive architecture\n",
    "# =========================================\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, width, out_dim, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else width, width))\n",
    "            layers.append(nn.LayerNorm(width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(width, out_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class LatentAdditive(nn.Module):\n",
    "    def __init__(self, n_genes, n_perts, n_covariates_enc, n_covariates_dec,\n",
    "                 latent_dim=160, encoder_width=3072, n_layers=3,\n",
    "                 dropout=0.1, softplus_output=True):\n",
    "        super().__init__()\n",
    "        self.gene_encoder = MLP(n_genes + n_covariates_enc, encoder_width, latent_dim,\n",
    "                                n_layers, dropout)\n",
    "        self.pert_encoder = MLP(n_perts, encoder_width, latent_dim,\n",
    "                                n_layers, dropout)\n",
    "        self.decoder      = MLP(latent_dim + n_covariates_dec, encoder_width, n_genes,\n",
    "                                n_layers, dropout)\n",
    "        self.softplus_output = softplus_output\n",
    "\n",
    "    def forward(self, x, p, cov_enc, cov_dec):\n",
    "        latent_ctrl = self.gene_encoder(torch.cat([x, cov_enc], dim=1))\n",
    "        latent_pert = self.pert_encoder(p)\n",
    "        latent_sum  = latent_ctrl + latent_pert\n",
    "        out = self.decoder(torch.cat([latent_sum, cov_dec], dim=1))\n",
    "        if self.softplus_output:\n",
    "            out = F.softplus(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6️⃣ Initialize model & optimizer\n",
    "# =========================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "n_genes = X_train.shape[1]\n",
    "n_covariates = cov_train.shape[1]\n",
    "\n",
    "model = LatentAdditive(\n",
    "    n_genes=n_genes,\n",
    "    n_perts=n_perts,\n",
    "    n_covariates_enc=n_covariates,\n",
    "    n_covariates_dec=n_covariates,\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4,\n",
    "                        weight_decay=1e-6, betas=(0.9, 0.999))\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\",\n",
    "                                                   factor=0.1, patience=5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 7️⃣ Training loop with AMP, val set\n",
    "# =========================================\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for xb, pb, cenc, cdec in tqdm(train_loader,\n",
    "                                   desc=f\"Epoch {epoch+1}/{n_epochs} (train)\"):\n",
    "        xb, pb, cenc, cdec = xb.to(device), pb.to(device), cenc.to(device), cdec.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            recon = model(xb, p=pb, cov_enc=cenc, cov_dec=cdec)\n",
    "            loss = F.mse_loss(recon, xb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, pb, cenc, cdec in tqdm(val_loader,\n",
    "                                       desc=f\"Epoch {epoch+1}/{n_epochs} (val)\"):\n",
    "            xb, pb, cenc, cdec = xb.to(device), pb.to(device), cenc.to(device), cdec.to(device)\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                recon = model(xb, p=pb, cov_enc=cenc, cov_dec=cdec)\n",
    "                loss = F.mse_loss(recon, xb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    sched.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | train={train_loss:.6f} | val={val_loss:.6f} | lr={opt.param_groups[0]['lr']:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/junxif/xin_lab\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'multiome.eval' from '/gpfs/home/junxif/xin_lab/perturbench/multiome/eval.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "# add parent directory of the notebook (perturbench/)\n",
    "sys.path.append(os.path.abspath(\"perturbench\"))\n",
    "from multiome.eval import evaluate_model\n",
    "import importlib\n",
    "import multiome.eval\n",
    "importlib.reload(multiome.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/junxif/xin_lab/perturbench/multiome/eval.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  \"per_pert\": { pert_name: {...}, ... }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'global': {'rmse': 0.2246228338438571,\n",
       "  'pearson_mean': 0.8598196506500244,\n",
       "  'cosine_mean': 0.8714019443546404,\n",
       "  'global_logfc_cosine': 0.0},\n",
       " 'per_pert': {'BCL11B': {'n_cells': 1380,\n",
       "   'logfc_corr': 0.7387996912002563,\n",
       "   'logfc_cosine': 0.7402163281244087,\n",
       "   'top50_recall': 0.08},\n",
       "  'XPO7': {'n_cells': 392,\n",
       "   'logfc_corr': 0.6945080757141113,\n",
       "   'logfc_cosine': 0.7193422239942252,\n",
       "   'top50_recall': 0.06},\n",
       "  'TBR1': {'n_cells': 607,\n",
       "   'logfc_corr': 0.7969701886177063,\n",
       "   'logfc_cosine': 0.7666682400719369,\n",
       "   'top50_recall': 0.24},\n",
       "  'CX3CL1': {'n_cells': 183,\n",
       "   'logfc_corr': 0.7026352882385254,\n",
       "   'logfc_cosine': 0.6510339120557519,\n",
       "   'top50_recall': 0.58},\n",
       "  'HERC1': {'n_cells': 162,\n",
       "   'logfc_corr': 0.6920470595359802,\n",
       "   'logfc_cosine': 0.65323934777674,\n",
       "   'top50_recall': 0.7},\n",
       "  'ctrl': {'n_cells': 309,\n",
       "   'logfc_corr': 0.29403549432754517,\n",
       "   'logfc_cosine': 0.23814074675678354,\n",
       "   'top50_recall': 0.1}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluate_model(model, test_loader, test_adata, device=device, k=50)\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "get",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
