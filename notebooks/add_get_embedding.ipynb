{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded AnnData: AnnData object with n_obs √ó n_vars = 21700 √ó 5000\n",
      "    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'seurat_clusters', 'Assign', 'scds', 'cxds', 'bcds', 'Sample', 'nCount_refAssay', 'nFeature_refAssay', 'predicted.subclass.score', 'predicted.subclass', 'CT', 'mito', 'BioSamp', 'CT2', 'ForPlot', 'Remove', 'active_ident', 'Assign_clean', 'condition', 'cell_type', 'cell_class', 'celltype_mapped'\n",
      "    var: 'variable_gene', 'gene_name', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
      "    uns: 'ATAC_embeddings', 'GET_embeddings', 'hvg', 'log1p'\n",
      "Split counts:\n",
      "split\n",
      "train    16070\n",
      "test      3033\n",
      "val       2597\n",
      "Name: count, dtype: int64\n",
      "Train: 16070 | Val: 2597 | Test: 3033\n",
      "Genes: 5000\n",
      "Perturbations: ['ANK3', 'BCL11B', 'CUL1', 'CX3CL1', 'DAB1', 'HERC1', 'RB1CC1', 'SATB2', 'TBR1', 'TRIO', 'XPO7', 'ctrl']\n",
      "Pert dim: 12\n",
      "Covariates dim: 4\n",
      "Preparing GET embeddings...\n",
      "Detected GET embedding: 5000 genes √ó 768-dim\n",
      "GET shapes:\n",
      "  GET_train: torch.Size([16070, 5000, 768])\n",
      "  GET_val:   torch.Size([2597, 5000, 768])\n",
      "  GET_test:  torch.Size([3033, 5000, 768])\n",
      "DataLoaders (with GET) ready.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "LatentAdditiveGET data loading with external train/val/test split.\n",
    "Includes:\n",
    "  - gene expression (X)\n",
    "  - perturbation one-hot (p)\n",
    "  - celltype covariates (cov)\n",
    "  - GET gene-level embeddings (GET)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================================\n",
    "# 0Ô∏è‚É£ Reproducibility\n",
    "# =========================================\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =========================================\n",
    "# 1Ô∏è‚É£ Load AnnData WITH GET embeddings\n",
    "# =========================================\n",
    "adata_path = \"/gpfs/home/junxif/xin_lab/perturbench/data/boli_anndata/boli_with_GETembedding_celltypeaware_filled.h5ad\"\n",
    "split_path = \"/gpfs/home/junxif/xin_lab/perturbench/data/boli_251006_1_qual_high_amt_high_split.csv\"\n",
    "\n",
    "adata = sc.read_h5ad(adata_path)\n",
    "print(\"Loaded AnnData:\", adata)\n",
    "\n",
    "# =========================================\n",
    "# 2Ô∏è‚É£ Load split CSV\n",
    "# =========================================\n",
    "df_split = pd.read_csv(split_path, header=None, names=[\"barcode\", \"split\"])\n",
    "df_split.index = df_split[\"barcode\"]\n",
    "\n",
    "adata.obs[\"split\"] = adata.obs.index.map(df_split[\"split\"])\n",
    "print(\"Split counts:\")\n",
    "print(adata.obs[\"split\"].value_counts())\n",
    "\n",
    "# =========================================\n",
    "# 3Ô∏è‚É£ Build train / val / test sets\n",
    "# =========================================\n",
    "train_adata = adata[adata.obs[\"split\"] == \"train\"].copy()\n",
    "val_adata   = adata[adata.obs[\"split\"] == \"val\"].copy()\n",
    "test_adata  = adata[adata.obs[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(f\"Train: {train_adata.n_obs} | Val: {val_adata.n_obs} | Test: {test_adata.n_obs}\")\n",
    "print(\"Genes:\", adata.n_vars)\n",
    "\n",
    "# =========================================\n",
    "# 4Ô∏è‚É£ Convert expression matrix to numpy\n",
    "# =========================================\n",
    "def to_numpy(X):\n",
    "    return X.toarray() if not isinstance(X, np.ndarray) else X\n",
    "\n",
    "X_train = torch.tensor(to_numpy(train_adata.X), dtype=torch.float32)\n",
    "X_val   = torch.tensor(to_numpy(val_adata.X),   dtype=torch.float32)\n",
    "X_test  = torch.tensor(to_numpy(test_adata.X),  dtype=torch.float32)\n",
    "\n",
    "# =========================================\n",
    "# 5Ô∏è‚É£ Perturbation one-hot\n",
    "# =========================================\n",
    "pert = adata.obs[\"condition\"].astype(\"category\")\n",
    "pert_onehot = pd.get_dummies(pert)\n",
    "\n",
    "p_train = torch.tensor(pert_onehot.loc[train_adata.obs.index].values, dtype=torch.float32)\n",
    "p_val   = torch.tensor(pert_onehot.loc[val_adata.obs.index].values,   dtype=torch.float32)\n",
    "p_test  = torch.tensor(pert_onehot.loc[test_adata.obs.index].values,  dtype=torch.float32)\n",
    "\n",
    "n_perts = p_train.shape[1]\n",
    "print(\"Perturbations:\", list(pert_onehot.columns))\n",
    "print(\"Pert dim:\", n_perts)\n",
    "\n",
    "# =========================================\n",
    "# 6Ô∏è‚É£ Celltype covariates one-hot\n",
    "# =========================================\n",
    "celltypes = adata.obs[\"celltype_mapped\"].astype(\"category\")\n",
    "cov_onehot = pd.get_dummies(celltypes)\n",
    "\n",
    "cov_train = torch.tensor(cov_onehot.loc[train_adata.obs.index].values, dtype=torch.float32)\n",
    "cov_val   = torch.tensor(cov_onehot.loc[val_adata.obs.index].values,   dtype=torch.float32)\n",
    "cov_test  = torch.tensor(cov_onehot.loc[test_adata.obs.index].values,  dtype=torch.float32)\n",
    "\n",
    "n_cov = cov_train.shape[1]\n",
    "print(\"Covariates dim:\", n_cov)\n",
    "\n",
    "# encoder/decoder use same covariates\n",
    "cov_train_enc = cov_train\n",
    "cov_train_dec = cov_train\n",
    "cov_val_enc   = cov_val\n",
    "cov_val_dec   = cov_val\n",
    "cov_test_enc  = cov_test\n",
    "cov_test_dec  = cov_test\n",
    "\n",
    "# =========================================\n",
    "# 7Ô∏è‚É£ Prepare GET gene-level embeddings\n",
    "# =========================================\n",
    "print(\"Preparing GET embeddings...\")\n",
    "\n",
    "# Extract GET dictionary\n",
    "get_dict = adata.uns[\"GET_embeddings\"]\n",
    "\n",
    "# Check gene dimension\n",
    "example_ct = list(get_dict.keys())[0]\n",
    "n_genes, d_get = get_dict[example_ct].shape\n",
    "assert n_genes == adata.n_vars, \"Mismatch GET vs adata gene count!\"\n",
    "\n",
    "print(f\"Detected GET embedding: {n_genes} genes √ó {d_get}-dim\")\n",
    "\n",
    "# Function to map GET embeddings for each cell in a subset\n",
    "def extract_get_for_subset(sub_adata, get_dict):\n",
    "    N = sub_adata.n_obs\n",
    "    celltypes = sub_adata.obs[\"celltype_mapped\"].astype(str).values\n",
    "    \n",
    "    GET_mat = np.zeros((N, n_genes, d_get), dtype=np.float32)\n",
    "    for i, ct in enumerate(celltypes):\n",
    "        if ct not in get_dict:\n",
    "            raise ValueError(f\"Missing GET embedding for cell type '{ct}'\")\n",
    "        GET_mat[i] = get_dict[ct]  # shape (genes √ó d_get)\n",
    "    return GET_mat\n",
    "\n",
    "# Build GET train/val/test\n",
    "GET_train = torch.tensor(extract_get_for_subset(train_adata, get_dict), dtype=torch.float32)\n",
    "GET_val   = torch.tensor(extract_get_for_subset(val_adata,   get_dict), dtype=torch.float32)\n",
    "GET_test  = torch.tensor(extract_get_for_subset(test_adata,  get_dict), dtype=torch.float32)\n",
    "\n",
    "print(\"GET shapes:\")\n",
    "print(\"  GET_train:\", GET_train.shape)\n",
    "print(\"  GET_val:  \", GET_val.shape)\n",
    "print(\"  GET_test: \", GET_test.shape)\n",
    "\n",
    "# =========================================\n",
    "# 8Ô∏è‚É£ Build PyTorch datasets (WITH GET)\n",
    "# =========================================\n",
    "train_ds = TensorDataset(\n",
    "    X_train,\n",
    "    p_train,\n",
    "    cov_train_enc,\n",
    "    cov_train_dec,\n",
    "    GET_train\n",
    ")\n",
    "\n",
    "val_ds = TensorDataset(\n",
    "    X_val,\n",
    "    p_val,\n",
    "    cov_val_enc,\n",
    "    cov_val_dec,\n",
    "    GET_val\n",
    ")\n",
    "\n",
    "test_ds = TensorDataset(\n",
    "    X_test,\n",
    "    p_test,\n",
    "    cov_test_enc,\n",
    "    cov_test_dec,\n",
    "    GET_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders (with GET) ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying GET mapping for train set...\n",
      "  Cell 6851 | celltype=cr_glut         | GET match = True\n",
      "  Cell 4631 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 2489 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 4484 | celltype=ctx-mge_gaba    | GET match = True\n",
      "  Cell  219 | celltype=nonit_glut      | GET match = True\n",
      "‚úî Passed 5/5 checks for train set.\n",
      "\n",
      "\n",
      "Verifying GET mapping for val set...\n",
      "  Cell 2584 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 2052 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 1577 | celltype=nonit_glut      | GET match = True\n",
      "  Cell   62 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 1172 | celltype=it_glut         | GET match = True\n",
      "‚úî Passed 5/5 checks for val set.\n",
      "\n",
      "\n",
      "Verifying GET mapping for test set...\n",
      "  Cell  393 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 2450 | celltype=it_glut         | GET match = True\n",
      "  Cell 2255 | celltype=it_glut         | GET match = True\n",
      "  Cell  271 | celltype=nonit_glut      | GET match = True\n",
      "  Cell 2541 | celltype=ctx-mge_gaba    | GET match = True\n",
      "‚úî Passed 5/5 checks for test set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# üîç Verification: check GET embeddings assigned correctly\n",
    "# ======================================================\n",
    "\n",
    "import random\n",
    "\n",
    "def verify_get_subset(sub_adata, GET_tensor, name=\"train\"):\n",
    "    print(f\"\\nVerifying GET mapping for {name} set...\")\n",
    "    \n",
    "    N = sub_adata.n_obs\n",
    "    idxs = random.sample(range(N), k=min(5, N))  # check 5 random cells\n",
    "    ok_count = 0\n",
    "    \n",
    "    for i in idxs:\n",
    "        ct = sub_adata.obs[\"celltype_mapped\"].iloc[i]\n",
    "        expected = get_dict[ct]                                    # numpy array\n",
    "        loaded   = GET_tensor[i].cpu().numpy()                     # what we stored\n",
    "        \n",
    "        same = np.allclose(expected, loaded, atol=1e-6)\n",
    "        \n",
    "        print(f\"  Cell {i:4d} | celltype={ct:15s} | GET match = {same}\")\n",
    "        if same:\n",
    "            ok_count += 1\n",
    "    \n",
    "    print(f\"‚úî Passed {ok_count}/{len(idxs)} checks for {name} set.\\n\")\n",
    "\n",
    "\n",
    "verify_get_subset(train_adata, GET_train, \"train\")\n",
    "verify_get_subset(val_adata,   GET_val,   \"val\")\n",
    "verify_get_subset(test_adata,  GET_test,  \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16606/2119253915.py:140: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/20 (train):   0%|          | 0/251 [00:00<?, ?it/s]/tmp/ipykernel_16606/2119253915.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/20 (train): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 251/251 [06:59<00:00,  1.67s/it]\n",
      "Epoch 1/20 (val):   0%|          | 0/41 [00:00<?, ?it/s]/tmp/ipykernel_16606/2119253915.py:192: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/20 (val): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [00:55<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train=0.068595 | val=0.046865 | lr=1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 (train):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 198/251 [06:19<01:43,  1.95s/it]"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5Ô∏è‚É£ LatentAdditiveGET architecture (with GET fusion)\n",
    "# =========================================\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, width, out_dim, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(in_dim if i == 0 else width, width))\n",
    "            layers.append(nn.LayerNorm(width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(width, out_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class LatentAdditiveGET(nn.Module):\n",
    "    \"\"\"\n",
    "    LatentAdditive + GET gene-level additive fusion (Strategy A)\n",
    "\n",
    "    forward(\n",
    "        x        : (batch √ó n_genes)\n",
    "        p        : (batch √ó n_perts)\n",
    "        cov_enc  : (batch √ó n_cov)\n",
    "        cov_dec  : (batch √ó n_cov)\n",
    "        get_emb  : (batch √ó n_genes √ó d_get)\n",
    "    )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_genes,\n",
    "        n_perts,\n",
    "        n_covariates_enc,\n",
    "        n_covariates_dec,\n",
    "        d_get,                  # GET embedding dimension\n",
    "        latent_dim=160,\n",
    "        encoder_width=3072,\n",
    "        n_layers=3,\n",
    "        dropout=0.1,\n",
    "        softplus_output=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoders\n",
    "        self.gene_encoder = MLP(\n",
    "            n_genes + n_covariates_enc,\n",
    "            encoder_width,\n",
    "            latent_dim,\n",
    "            n_layers,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        self.pert_encoder = MLP(\n",
    "            n_perts,\n",
    "            encoder_width,\n",
    "            latent_dim,\n",
    "            n_layers,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = MLP(\n",
    "            latent_dim + n_covariates_dec,\n",
    "            encoder_width,\n",
    "            n_genes,\n",
    "            n_layers,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "        # NEW: Linear GET ‚Üí per-gene scalar\n",
    "        # maps each d_get-dimensional gene embedding to a single scalar\n",
    "        self.linear_get = nn.Linear(d_get, 1)\n",
    "\n",
    "        self.softplus_output = softplus_output\n",
    "\n",
    "    def forward(self, x, p, cov_enc, cov_dec, get_emb):\n",
    "        \"\"\"\n",
    "        get_emb: (batch √ó n_genes √ó d_get)\n",
    "        \"\"\"\n",
    "        # ----- latent computation -----\n",
    "        latent_ctrl = self.gene_encoder(torch.cat([x, cov_enc], dim=1))\n",
    "        latent_pert = self.pert_encoder(p)\n",
    "        latent_sum  = latent_ctrl + latent_pert  # (batch √ó latent_dim)\n",
    "\n",
    "        # ----- base decoder output -----\n",
    "        zcat = torch.cat([latent_sum, cov_dec], dim=1)\n",
    "        base_out = self.decoder(zcat)  # (batch √ó n_genes)\n",
    "\n",
    "        # ----- GET additive term -----\n",
    "        # get_emb: (batch √ó n_genes √ó d_get)\n",
    "        # linear_get -> (batch √ó n_genes √ó 1)\n",
    "        get_term = self.linear_get(get_emb).squeeze(-1)\n",
    "\n",
    "        # ----- combine base prediction + GET -----\n",
    "        out = base_out + get_term\n",
    "\n",
    "        if self.softplus_output:\n",
    "            out = F.softplus(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 6Ô∏è‚É£ Initialize model & optimizer\n",
    "# =========================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "n_genes = X_train.shape[1]\n",
    "n_covariates = cov_train.shape[1]\n",
    "\n",
    "# GET embedding dimension\n",
    "_, _, d_get = GET_train.shape\n",
    "\n",
    "model = LatentAdditiveGET(\n",
    "    n_genes=n_genes,\n",
    "    n_perts=n_perts,\n",
    "    n_covariates_enc=n_covariates,\n",
    "    n_covariates_dec=n_covariates,\n",
    "    d_get=d_get,                   # <---- NEW\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, mode=\"min\", factor=0.1, patience=5\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# 7Ô∏è‚É£ Training loop with GET embeddings\n",
    "# =========================================\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "\n",
    "    for xb, pb, cenc, cdec, getb in tqdm(\n",
    "        train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} (train)\"\n",
    "    ):\n",
    "        xb   = xb.to(device)\n",
    "        pb   = pb.to(device)\n",
    "        cenc = cenc.to(device)\n",
    "        cdec = cdec.to(device)\n",
    "        getb = getb.to(device)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            recon = model(xb, p=pb, cov_enc=cenc, cov_dec=cdec, get_emb=getb)\n",
    "            loss = F.mse_loss(recon, xb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, pb, cenc, cdec, getb in tqdm(\n",
    "            val_loader, desc=f\"Epoch {epoch+1}/{n_epochs} (val)\"\n",
    "        ):\n",
    "            xb   = xb.to(device)\n",
    "            pb   = pb.to(device)\n",
    "            cenc = cenc.to(device)\n",
    "            cdec = cdec.to(device)\n",
    "            getb = getb.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                recon = model(xb, p=pb, cov_enc=cenc, cov_dec=cdec, get_emb=getb)\n",
    "                loss = F.mse_loss(recon, xb)\n",
    "\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    sched.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | train={train_loss:.6f} | val={val_loss:.6f} | lr={opt.param_groups[0]['lr']:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"/gpfs/home/junxif/xin_lab/multiome\"))\n",
    "from utils.eval import evaluate_model\n",
    "import importlib\n",
    "import utils.eval\n",
    "importlib.reload(utils.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, test_loader, test_adata, device=device, k=50)\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "get",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
